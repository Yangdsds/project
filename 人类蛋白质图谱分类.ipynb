{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSettings:\n",
    "    \n",
    "    def __init__(self, fit_baseline=False,\n",
    "                 fit_improved_baseline=True,\n",
    "                 fit_improved_higher_batchsize=False,\n",
    "                 fit_improved_without_dropout=False):\n",
    "        self.fit_baseline = fit_baseline\n",
    "        self.fit_improved_baseline = fit_improved_baseline\n",
    "        self.fit_improved_higher_batchsize = fit_improved_higher_batchsize\n",
    "        self.fit_improved_without_dropout = fit_improved_without_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelsettings = KernelSettings(fit_baseline=False,\n",
    "                                fit_improved_baseline=False,\n",
    "                                fit_improved_higher_batchsize=False,\n",
    "                                fit_improved_without_dropout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dropout=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from scipy.misc import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"../input/human-protein-atlas-image-classification/train.csv\")\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"../input/human-protein-atlas-image-classification/sample_submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = submission.Id.values\n",
    "print(len(test_names))\n",
    "print(test_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}\n",
    "\n",
    "reverse_train_labels = dict((v,k) for k,v in label_names.items())\n",
    "\n",
    "def fill_targets(row):\n",
    "    row.Target = np.array(row.Target.split(\" \")).astype(np.int)\n",
    "    for num in row.Target:\n",
    "        name = label_names[int(num)]\n",
    "        row.loc[name] = 1\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in label_names.keys():\n",
    "    train_labels[label_names[key]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.apply(fill_targets, axis=1)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.DataFrame(data=test_names, columns=[\"Id\"])\n",
    "for col in train_labels.columns.values:\n",
    "    if col != \"Id\":\n",
    "        test_labels[col] = 0\n",
    "test_labels.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=0).sort_values(ascending=False)\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[\"number_of_targets\"] = train_labels.drop([\"Id\", \"Target\"],axis=1).sum(axis=1)\n",
    "count_perc = np.round(100 * train_labels[\"number_of_targets\"].value_counts() / train_labels.shape[0], 2)\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Reds\")\n",
    "plt.xlabel(\"Number of targets per image\")\n",
    "plt.ylabel(\"% of train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(train_labels[train_labels.number_of_targets>1].drop(\n",
    "    [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n",
    ").corr(), cmap=\"RdYlBu\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_counts(special_target, labels):\n",
    "    counts = labels[labels[special_target] == 1].drop(\n",
    "        [\"Id\", \"Target\", \"number_of_targets\"],axis=1\n",
    "    ).sum(axis=0)\n",
    "    counts = counts[counts > 0]\n",
    "    counts = counts.sort_values()\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyso_endo_counts = find_counts(\"Lysosomes\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "sns.barplot(x=lyso_endo_counts.index.values, y=lyso_endo_counts.values, palette=\"Blues\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rod_rings_counts = find_counts(\"Rods & rings\", train_labels)\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=rod_rings_counts.index.values, y=rod_rings_counts.values, palette=\"Greens\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peroxi_counts = find_counts(\"Peroxisomes\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=peroxi_counts.index.values, y=peroxi_counts.values, palette=\"Reds\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tubeends_counts = find_counts(\"Microtubule ends\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=tubeends_counts.index.values, y=tubeends_counts.values, palette=\"Purples\")\n",
    "plt.ylabel(\"Counts in train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclear_speckles_counts = find_counts(\"Nuclear speckles\", train_labels)\n",
    "\n",
    "plt.figure(figsize=(15,3))\n",
    "sns.barplot(x=nuclear_speckles_counts.index.values, y=nuclear_speckles_counts.values, palette=\"Oranges\")\n",
    "plt.xticks(rotation=\"70\")\n",
    "plt.ylabel(\"Counts in train data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "files = listdir(\"../input/human-protein-atlas-image-classification/train\")\n",
    "for n in range(10):\n",
    "    print(files[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files) / 4 == train_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../input/human-protein-atlas-image-classification/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(basepath, image_id):\n",
    "    images = np.zeros(shape=(4,512,512))\n",
    "    images[0,:,:] = imread(basepath + image_id + \"_green\" + \".png\")\n",
    "    images[1,:,:] = imread(basepath + image_id + \"_red\" + \".png\")\n",
    "    images[2,:,:] = imread(basepath + image_id + \"_blue\" + \".png\")\n",
    "    images[3,:,:] = imread(basepath + image_id + \"_yellow\" + \".png\")\n",
    "    return images\n",
    "\n",
    "def make_image_row(image, subax, title):\n",
    "    subax[0].imshow(image[0], cmap=\"Greens\")\n",
    "    subax[1].imshow(image[1], cmap=\"Reds\")\n",
    "    subax[1].set_title(\"stained microtubules\")\n",
    "    subax[2].imshow(image[2], cmap=\"Blues\")\n",
    "    subax[2].set_title(\"stained nucleus\")\n",
    "    subax[3].imshow(image[3], cmap=\"Oranges\")\n",
    "    subax[3].set_title(\"stained endoplasmatic reticulum\")\n",
    "    subax[0].set_title(title)\n",
    "    return subax\n",
    "\n",
    "def make_title(file_id):\n",
    "    file_targets = train_labels.loc[train_labels.Id==file_id, \"Target\"].values[0]\n",
    "    title = \" - \"\n",
    "    for n in file_targets:\n",
    "        title += label_names[n] + \" - \"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetGroupIterator:\n",
    "    \n",
    "    def __init__(self, target_names, batch_size, basepath):\n",
    "        self.target_names = target_names\n",
    "        self.target_list = [reverse_train_labels[key] for key in target_names]\n",
    "        self.batch_shape = (batch_size, 4, 512, 512)\n",
    "        self.basepath = basepath\n",
    "    \n",
    "    def find_matching_data_entries(self):\n",
    "        train_labels[\"check_col\"] = train_labels.Target.apply(\n",
    "            lambda l: self.check_subset(l)\n",
    "        )\n",
    "        self.images_identifier = train_labels[train_labels.check_col==1].Id.values\n",
    "        train_labels.drop(\"check_col\", axis=1, inplace=True)\n",
    "    \n",
    "    def check_subset(self, targets):\n",
    "        return np.where(set(self.target_list).issubset(set(targets)), 1, 0)\n",
    "    \n",
    "    def get_loader(self):\n",
    "        filenames = []\n",
    "        idx = 0\n",
    "        images = np.zeros(self.batch_shape)\n",
    "        for image_id in self.images_identifier:\n",
    "            images[idx,:,:,:] = load_image(self.basepath, image_id)\n",
    "            filenames.append(image_id)\n",
    "            idx += 1\n",
    "            if idx == self.batch_shape[0]:\n",
    "                yield filenames, images\n",
    "                filenames = []\n",
    "                images = np.zeros(self.batch_shape)\n",
    "                idx = 0\n",
    "        if idx > 0:\n",
    "            yield filenames, images\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_choice = [\"Lysosomes\", \"Endosomes\"]\n",
    "your_batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageloader = TargetGroupIterator(your_choice, your_batch_size, train_path)\n",
    "imageloader.find_matching_data_entries()\n",
    "iterator = imageloader.get_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids, images = next(iterator)\n",
    "\n",
    "fig, ax = plt.subplots(len(file_ids),4,figsize=(20,5*len(file_ids)))\n",
    "if ax.shape == (4,):\n",
    "    ax = ax.reshape(1,-1)\n",
    "for n in range(len(file_ids)):\n",
    "    make_image_row(images[n], ax[n], make_title(file_ids[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_files = listdir(\"../input/human-protein-atlas-image-classification/train\")\n",
    "test_files = listdir(\"../input/human-protein-atlas-image-classification/test\")\n",
    "percentage = np.round(len(test_files) / len(train_files) * 100)\n",
    "\n",
    "print(\"The test set size turns out to be {} % compared to the train set.\".format(percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "splitter = RepeatedKFold(n_splits=3, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "\n",
    "for train_idx, test_idx in splitter.split(train_labels.index.values):\n",
    "    partition = {}\n",
    "    partition[\"train\"] = train_labels.Id.values[train_idx]\n",
    "    partition[\"validation\"] = train_labels.Id.values[test_idx]\n",
    "    partitions.append(partition)\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    print(\"TRAIN:\", len(train_idx), \"TEST:\", len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions[0][\"train\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameter:\n",
    "    \n",
    "    def __init__(self, basepath,\n",
    "                 num_classes=28,\n",
    "                 image_rows=512,\n",
    "                 image_cols=512,\n",
    "                 batch_size=200,\n",
    "                 n_channels=1,\n",
    "                 row_scale_factor=4,\n",
    "                 col_scale_factor=4,\n",
    "                 shuffle=False,\n",
    "                 n_epochs=1):\n",
    "        self.basepath = basepath\n",
    "        self.num_classes = num_classes\n",
    "        self.image_rows = image_rows\n",
    "        self.image_cols = image_cols\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.row_scale_factor = row_scale_factor\n",
    "        self.col_scale_factor = col_scale_factor\n",
    "        self.scaled_row_dim = np.int(self.image_rows / self.row_scale_factor)\n",
    "        self.scaled_col_dim = np.int(self.image_cols / self.col_scale_factor)\n",
    "        self.n_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameter(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.parameter = modelparameter\n",
    "        self.basepath = self.parameter.basepath\n",
    "        self.scaled_row_dim = self.parameter.scaled_row_dim\n",
    "        self.scaled_col_dim = self.parameter.scaled_col_dim\n",
    "        self.n_channels = self.parameter.n_channels\n",
    "    \n",
    "    def preprocess(self, image):\n",
    "        image = self.resize(image)\n",
    "        image = self.reshape(image)\n",
    "        image = self.normalize(image)\n",
    "        return image\n",
    "    \n",
    "    def resize(self, image):\n",
    "        image = resize(image, (self.scaled_row_dim, self.scaled_col_dim))\n",
    "        return image\n",
    "    \n",
    "    def reshape(self, image):\n",
    "        image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "        return image\n",
    "    \n",
    "    def normalize(self, image):\n",
    "        image /= 255 \n",
    "        return image\n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        image = np.zeros(shape=(512,512,4))\n",
    "        image[:,:,0] = imread(self.basepath + image_id + \"_green\" + \".png\")\n",
    "        image[:,:,1] = imread(self.basepath + image_id + \"_blue\" + \".png\")\n",
    "        image[:,:,2] = imread(self.basepath + image_id + \"_red\" + \".png\")\n",
    "        image[:,:,3] = imread(self.basepath + image_id + \"_yellow\" + \".png\")\n",
    "        return image[:,:,0:self.parameter.n_channels]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ImagePreprocessor(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = images[0,0]\n",
    "preprocessed = preprocessor.preprocess(example)\n",
    "print(example.shape)\n",
    "print(preprocessed.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,10))\n",
    "ax[0].imshow(example, cmap=\"Greens\")\n",
    "ax[1].imshow(preprocessed.reshape(parameter.scaled_row_dim,parameter.scaled_col_dim), cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor):\n",
    "        self.current_epoch = 0\n",
    "        self.params = modelparameter\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dim = (self.params.scaled_row_dim, self.params.scaled_col_dim)\n",
    "        self.batch_size = self.params.batch_size\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.shuffle = self.params.shuffle\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes, random_state=self.current_epoch)\n",
    "            self.current_epoch += 1\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier].drop(\n",
    "                [\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, self.num_classes), dtype=int)\n",
    "        # Generate data\n",
    "        for i, identifier in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            image = self.preprocessor.load_image(identifier)\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            X[i] = image\n",
    "            # Store class\n",
    "            y[i] = self.get_targets_per_image(identifier)\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictGenerator:\n",
    "    \n",
    "    def __init__(self, predict_Ids, imagepreprocessor, predict_path):\n",
    "        self.preprocessor = imagepreprocessor\n",
    "        self.preprocessor.basepath = predict_path\n",
    "        self.identifiers = predict_Ids\n",
    "    \n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.identifiers), self.preprocessor.parameter.num_classes))\n",
    "        for n in range(len(self.identifiers)):\n",
    "            image = self.preprocessor.load_image(self.identifiers[n])\n",
    "            image = self.preprocessor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.initializers import VarianceScaling\n",
    "\n",
    "\n",
    "class BaseLineModel:\n",
    "    \n",
    "    def __init__(self, modelparameter):\n",
    "        self.params = modelparameter\n",
    "        self.num_classes = self.params.num_classes\n",
    "        self.img_rows = self.params.scaled_row_dim\n",
    "        self.img_cols = self.params.scaled_col_dim\n",
    "        self.n_channels = self.params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.my_metrics = ['accuracy']\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape,\n",
    "                             kernel_initializer=VarianceScaling(seed=0)))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                             kernel_initializer=VarianceScaling(seed=0)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu',\n",
    "                            kernel_initializer=VarianceScaling(seed=0),))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=self.my_metrics)\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "    \n",
    "    def learn(self):\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8)\n",
    "    \n",
    "    def score(self):\n",
    "        return self.model.evaluate_generator(generator=self.validation_generator,\n",
    "                                      use_multiprocessing=True, \n",
    "                                      workers=8)\n",
    "    \n",
    "    def predict(self, predict_generator):\n",
    "        y = predict_generator.predict(self.model)\n",
    "        return y\n",
    "    \n",
    "    def save(self, modeloutputpath):\n",
    "        self.model.save(modeloutputpath)\n",
    "    \n",
    "    def load(self, modelinputpath):\n",
    "        self.model = load_model(modelinputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "partition = partitions[0]\n",
    "labels = train_labels\n",
    "\n",
    "print(\"Number of samples in train: {}\".format(len(partition[\"train\"])))\n",
    "print(\"Number of samples in validation: {}\".format(len(partition[\"validation\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(partition['train'], labels, parameter, preprocessor)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, parameter, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessor = ImagePreprocessor(parameter)\n",
    "submission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "target_names = train_labels.drop([\"Target\", \"number_of_targets\", \"Id\"], axis=1).columns\n",
    "\n",
    "if kernelsettings.fit_baseline == True:\n",
    "    model = BaseLineModel(parameter)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    history = model.learn()\n",
    "    \n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    baseline_proba_predictions = pd.DataFrame(index = partition['validation'],\n",
    "                                              data=proba_predictions,\n",
    "                                              columns=target_names)\n",
    "    baseline_proba_predictions.to_csv(\"baseline_predictions.csv\")\n",
    "    baseline_losses = pd.DataFrame(history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    baseline_losses[\"val_loss\"] = history.history[\"val_loss\"]\n",
    "    baseline_losses.to_csv(\"baseline_losses.csv\")\n",
    "    \n",
    "    \n",
    "    submission_proba_predictions = model.predict(submission_predict_generator)\n",
    "    baseline_labels = test_labels.copy()\n",
    "    baseline_labels.loc[:, test_labels.drop([\"Id\", \"Target\"], axis=1).columns.values] = submission_proba_predictions\n",
    "    baseline_labels.to_csv(\"baseline_submission_proba.csv\")\n",
    "# If you already have done a baseline fit once, \n",
    "# you can load predictions as csv and further fitting is not neccessary:\n",
    "else:\n",
    "    baseline_proba_predictions = pd.read_csv(\"../input/protein-atlas-eab-predictions/baseline_predictions.csv\", index_col=0)\n",
    "    baseline_losses = pd.read_csv(\"../input/protein-atlas-eab-predictions/baseline_losses.csv\", index_col=0)\n",
    "    baseline_labels = pd.read_csv(\"../input/protein-atlas-eab-predictions/baseline_submission_proba.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels = train_labels.loc[train_labels.Id.isin(partition[\"validation\"])].copy()\n",
    "validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_proba_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "\n",
    "y_true = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values\n",
    "y_pred = np.where(baseline_proba_predictions.values > 0.5, 1, 0)\n",
    "\n",
    "accuracy(y_true.flatten(), y_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_predictions = baseline_proba_predictions.values\n",
    "hot_values = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).values.flatten()\n",
    "one_hot = (hot_values.sum()) / hot_values.shape[0] * 100\n",
    "zero_hot = (hot_values.shape[0] - hot_values.sum()) / hot_values.shape[0] * 100\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "sns.distplot(proba_predictions.flatten() * 100, color=\"DodgerBlue\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Probability in %\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[0].set_title(\"Predicted probabilities\")\n",
    "sns.barplot(x=[\"label = 0\", \"label = 1\"], y=[zero_hot, one_hot], ax=ax[1])\n",
    "ax[1].set_ylim([0,100])\n",
    "ax[1].set_title(\"True target label count\")\n",
    "ax[1].set_ylabel(\"Percentage\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = np.mean(proba_predictions, axis=0)\n",
    "std_predictions = np.std(proba_predictions, axis=0)\n",
    "mean_targets = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).mean()\n",
    "\n",
    "labels = validation_labels.drop([\"Id\", \"Target\", \"number_of_targets\"], axis=1).columns.values\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "sns.barplot(x=labels,\n",
    "            y=mean_predictions,\n",
    "            ax=ax[0])\n",
    "ax[0].set_xticklabels(labels=labels,\n",
    "                      rotation=90)\n",
    "ax[0].set_ylabel(\"Mean predicted probability\")\n",
    "ax[0].set_title(\"Mean predicted probability per class over all samples\")\n",
    "sns.barplot(x=labels,\n",
    "           y=std_predictions,\n",
    "           ax=ax[1])\n",
    "ax[1].set_xticklabels(labels=labels,\n",
    "                      rotation=90)\n",
    "ax[1].set_ylabel(\"Standard deviation\")\n",
    "ax[1].set_title(\"Standard deviation of predicted probability per class over all samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(20,5))\n",
    "sns.barplot(x=labels, y=mean_targets.values, ax=ax)\n",
    "ax.set_xticklabels(labels=labels,\n",
    "                      rotation=90)\n",
    "ax.set_ylabel(\"Percentage of hot (1)\")\n",
    "ax.set_title(\"Percentage of hot counts (ones) per target class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"Cytosol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.distplot(baseline_proba_predictions[feature].values[0:-10], color=\"Purple\")\n",
    "plt.xlabel(\"Predicted probabilites of {}\".format(feature))\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wishlist = [\"Nucleoplasm\", \"Cytosol\", \"Plasma membrane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDataGenerator(DataGenerator):\n",
    "    \n",
    "    # in contrast to the base DataGenerator we add a target wishlist to init\n",
    "    def __init__(self, list_IDs, labels, modelparameter, imagepreprocessor, target_wishlist):\n",
    "        super().__init__(list_IDs, labels, modelparameter, imagepreprocessor)\n",
    "        self.target_wishlist = target_wishlist\n",
    "    \n",
    "    def get_targets_per_image(self, identifier):\n",
    "        return self.labels.loc[self.labels.Id==identifier][self.target_wishlist].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def base_f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return f1\n",
    "\n",
    "def f1_min(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.min(f1)\n",
    "\n",
    "def f1_max(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.max(f1)\n",
    "\n",
    "def f1_mean(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_std(y_true, y_pred):\n",
    "    f1 = base_f1(y_true, y_pred)\n",
    "    return K.std(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(BaseLineModel):\n",
    "    \n",
    "    def __init__(self, modelparameter,\n",
    "                 use_dropout,\n",
    "                 my_metrics=[f1_mean, f1_std, f1_min, f1_max]):\n",
    "        \n",
    "        super().__init__(modelparameter)\n",
    "        self.my_metrics = my_metrics\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "    def learn(self):\n",
    "        self.history = TrackHistory()\n",
    "        return self.model.fit_generator(generator=self.training_generator,\n",
    "                    validation_data=self.validation_generator,\n",
    "                    epochs=self.params.n_epochs, \n",
    "                    use_multiprocessing=True,\n",
    "                    workers=8,\n",
    "                    callbacks = [self.history])\n",
    "    \n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape,\n",
    "                             kernel_initializer=VarianceScaling(seed=0),))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                             kernel_initializer=VarianceScaling(seed=0),))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        if self.use_dropout:\n",
    "            self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu',\n",
    "                            kernel_initializer=VarianceScaling(seed=0),))\n",
    "        if self.use_dropout:\n",
    "            self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=5, batch_size=64)\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = ImprovedDataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor, wishlist)\n",
    "validation_generator = ImprovedDataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor, wishlist)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessor = ImagePreprocessor(parameter)\n",
    "submission_predict_generator = PredictGenerator(test_names, test_preprocessor, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "if kernelsettings.fit_improved_baseline == True:\n",
    "    model = ImprovedModel(parameter, use_dropout=use_dropout)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    epoch_history = model.learn()\n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    #model.save(\"improved_model.h5\")\n",
    "    \n",
    "    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n",
    "    improved_proba_predictions.to_csv(\"improved_predictions.csv\")\n",
    "    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n",
    "    improved_losses.to_csv(\"improved_losses.csv\")\n",
    "    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n",
    "    improved_batch_losses.to_csv(\"improved_batch_losses.csv\")\n",
    "    \n",
    "    improved_submission_proba_predictions = model.predict(submission_predict_generator)\n",
    "    improved_test_labels = test_labels.copy()\n",
    "    improved_test_labels.loc[:, wishlist] = improved_submission_proba_predictions\n",
    "    improved_test_labels.to_csv(\"improved_submission_proba.csv\")\n",
    "# If you already have done a baseline fit once, \n",
    "# you can load predictions as csv and further fitting is not neccessary:\n",
    "else:\n",
    "    improved_proba_predictions = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_predictions.csv\", index_col=0)\n",
    "    improved_losses= pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_losses.csv\", index_col=0)\n",
    "    improved_batch_losses = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_batch_losses.csv\", index_col=0)\n",
    "    improved_test_labels = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_submission_proba.csv\",\n",
    "                                      index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,13))\n",
    "ax[0].plot(np.arange(1,6), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss\")\n",
    "ax[0].plot(np.arange(1,6), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Loss evolution per epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label=\"train_batch_losses\")\n",
    "ax[1].set_xlabel(\"Number of update steps in total\")\n",
    "ax[1].set_ylabel(\"Train loss\")\n",
    "ax[1].set_title(\"Train loss evolution per batch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(25,15))\n",
    "sns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\n",
    "ax[0].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\n",
    "ax[1].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\n",
    "ax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\n",
    "ax[2].set_xlim([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_test_labels.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_proba_predictions.set_index(partition[\"validation\"], inplace=True)\n",
    "improved_proba_predictions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_labels.set_index(\"Id\", inplace=True)\n",
    "validation_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(20,18))\n",
    "for n in range(len(wishlist)):\n",
    "    sns.distplot(improved_proba_predictions.loc[validation_labels[wishlist[n]] == 1,\n",
    "                                                wishlist[n]], color=\"Green\", label=\"1-hot\", ax=ax[n])\n",
    "    sns.distplot(improved_proba_predictions.loc[validation_labels[wishlist[n]] == 0,\n",
    "                                                wishlist[n]], color=\"Red\", label=\"0-zero\", ax=ax[n])\n",
    "    ax[n].set_title(wishlist[n])\n",
    "    ax[n].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th_cytosol = 0.4\n",
    "th_plasma_membrane = 0.2\n",
    "th_nucleoplasm = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_submission = improved_test_labels.copy()\n",
    "improved_submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_submission[\"Nucleoplasm\"] = np.where(improved_test_labels[\"Nucleoplasm\"] >= th_nucleoplasm, 1, 0)\n",
    "improved_submission[\"Cytosol\"] = np.where(improved_test_labels[\"Cytosol\"] >= th_cytosol, 1, 0)\n",
    "improved_submission[\"Plasma membrane\"] = np.where(\n",
    "    improved_test_labels[\"Plasma membrane\"] >= th_plasma_membrane, 1, 0)\n",
    "\n",
    "improved_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_target(row):\n",
    "    target_list = []\n",
    "    for col in validation_labels.drop([\"Target\", \"number_of_targets\"], axis=1).columns:\n",
    "        if row[col] == 1:\n",
    "            target_list.append(str(reverse_train_labels[col]))\n",
    "    if len(target_list) == 0:\n",
    "        return str(0)\n",
    "    return \" \".join(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_submission[\"Predicted\"] = improved_submission.apply(lambda l: transform_to_target(l), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = improved_submission.loc[:, [\"Id\", \"Predicted\"]]\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"improved_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = ModelParameter(train_path, num_classes=len(wishlist), n_epochs=10, batch_size=128)\n",
    "preprocessor = ImagePreprocessor(parameter)\n",
    "labels = train_labels\n",
    "\n",
    "training_generator = ImprovedDataGenerator(partition['train'], labels,\n",
    "                                           parameter, preprocessor, wishlist)\n",
    "validation_generator = ImprovedDataGenerator(partition['validation'], labels,\n",
    "                                             parameter, preprocessor, wishlist)\n",
    "predict_generator = PredictGenerator(partition['validation'], preprocessor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "if kernelsettings.fit_improved_higher_batchsize == True:\n",
    "    model = ImprovedModel(parameter, use_dropout=True)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    epoch_history = model.learn()\n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    #model.save(\"improved_model.h5\")\n",
    "    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n",
    "    improved_proba_predictions.to_csv(\"improved_hbatch_predictions.csv\")\n",
    "    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n",
    "    improved_losses.to_csv(\"improved_hbatch_losses.csv\")\n",
    "    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n",
    "    improved_batch_losses.to_csv(\"improved_hbatch_batch_losses.csv\")\n",
    "# If you already have done a baseline fit once, \n",
    "# you can load predictions as csv and further fitting is not neccessary:\n",
    "else:\n",
    "    improved_proba_predictions = pd.read_csv(\n",
    "        \"../input/protein-atlas-eab-predictions/improved_hbatch_predictions.csv\", index_col=0)\n",
    "    improved_losses= pd.read_csv(\n",
    "        \"../input/protein-atlas-eab-predictions/improved_hbatch_losses.csv\", index_col=0)\n",
    "    improved_batch_losses = pd.read_csv(\"../input/protein-atlas-eab-predictions/improved_hbatch_batch_losses.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,13))\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Loss evolution per epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(improved_batch_losses.batch_losses.values, 'r-+', label=\"train_batch_losses\")\n",
    "ax[1].set_xlabel(\"Number of update steps in total\")\n",
    "ax[1].set_ylabel(\"Train loss\")\n",
    "ax[1].set_title(\"Train loss evolution per batch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(25,15))\n",
    "sns.distplot(improved_proba_predictions.values[:,0], color=\"Orange\", ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[0]))\n",
    "ax[0].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,1], color=\"Purple\", ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[1]))\n",
    "ax[1].set_xlim([0,1])\n",
    "sns.distplot(improved_proba_predictions.values[:,2], color=\"Limegreen\", ax=ax[2])\n",
    "ax[2].set_xlabel(\"Predicted probabilites of {}\".format(improved_proba_predictions.columns.values[2]))\n",
    "ax[2].set_xlim([0,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(BaseLineModel):\n",
    "\n",
    "    def __init__(self, modelparameter,\n",
    "                 my_metrics=[f1_mean, f1_std, f1_min, f1_max],\n",
    "                 use_dropout=True):\n",
    "\n",
    "        super().__init__(modelparameter)\n",
    "        self.my_metrics = my_metrics\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=self.input_shape))\n",
    "        self.model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        if self.use_dropout:\n",
    "            self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(64, activation='relu'))\n",
    "        if self.use_dropout:\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(self.num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run computation and store results as csv\n",
    "if kernelsettings.fit_improved_without_dropout == True:\n",
    "    model = ImprovedModel(parameter, use_dropout=False)\n",
    "    model.build_model()\n",
    "    model.compile_model()\n",
    "    model.set_generators(training_generator, validation_generator)\n",
    "    epoch_history = model.learn()\n",
    "    proba_predictions = model.predict(predict_generator)\n",
    "    #model.save(\"improved_model.h5\")\n",
    "    improved_proba_predictions = pd.DataFrame(proba_predictions, columns=wishlist)\n",
    "    improved_proba_predictions.to_csv(\"improved_nodropout_predictions.csv\")\n",
    "    improved_losses = pd.DataFrame(epoch_history.history[\"loss\"], columns=[\"train_loss\"])\n",
    "    improved_losses[\"val_loss\"] = epoch_history.history[\"val_loss\"]\n",
    "    improved_losses.to_csv(\"improved_nodropout_losses.csv\")\n",
    "    improved_batch_losses = pd.DataFrame(model.history.losses, columns=[\"batch_losses\"])\n",
    "    improved_batch_losses.to_csv(\"improved_nodropout_batch_losses.csv\")\n",
    "# If you already have done a baseline fit once, \n",
    "# you can load predictions as csv and further fitting is not neccessary:\n",
    "else:\n",
    "    improved_proba_predictions_no_dropout = pd.read_csv(\n",
    "        \"../input/protein-atlas-eab-predictions/improved_nodropout_predictions.csv\", index_col=0)\n",
    "    improved_losses_no_dropout= pd.read_csv(\n",
    "        \"../input/protein-atlas-eab-predictions/improved_nodropout_losses.csv\", index_col=0)\n",
    "    improved_batch_losses_no_dropout = pd.read_csv(\n",
    "        \"../input/protein-atlas-eab-predictions/improved_nodropout_batch_losses.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,13))\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"train_loss\"].values, 'r--o', label=\"train_loss_dropout\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses_no_dropout[\"train_loss\"].values, 'r-o', label=\"train_loss_no_dropout\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses[\"val_loss\"].values, 'g--o', label=\"validation_loss\")\n",
    "ax[0].plot(np.arange(1,11), improved_losses_no_dropout[\"val_loss\"].values, 'g-o', label=\"validation_loss_no_dropout\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_title(\"Loss evolution per epoch\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(improved_batch_losses.batch_losses.values[-800::], 'r-+', label=\"train_batch_losses_dropout\")\n",
    "ax[1].plot(improved_batch_losses_no_dropout.batch_losses.values[-800::], 'b-+',\n",
    "           label=\"train_batch_losses_no_dropout\")\n",
    "ax[1].set_xlabel(\"Number of update steps in total\")\n",
    "ax[1].set_ylabel(\"Train loss\")\n",
    "ax[1].set_title(\"Train loss evolution per batch\");\n",
    "ax[1].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
